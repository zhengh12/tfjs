{"remainingRequest":"C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\node_modules\\babel-loader\\lib\\index.js!C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\node_modules\\eslint-loader\\index.js??ref--13-0!C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\src\\utils\\cnntfjs.js","dependencies":[{"path":"C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\src\\utils\\cnntfjs.js","mtime":1574150302576},{"path":"C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\node_modules\\cache-loader\\dist\\cjs.js","mtime":499162500000},{"path":"C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\node_modules\\babel-loader\\lib\\index.js","mtime":499162500000},{"path":"C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\node_modules\\eslint-loader\\index.js","mtime":499162500000}],"contextDependencies":[],"result":["import \"regenerator-runtime/runtime\";\nimport _asyncToGenerator from \"C:\\\\Users\\\\1\\\\Desktop\\\\tensorflowjs\\\\face-api-demo-vue-master\\\\node_modules\\\\@babel\\\\runtime-corejs2/helpers/esm/asyncToGenerator\";\n\nvar tf = require(\"@tensorflow/tfjs\");\n\nimport MnistData from \"./data\";\n\nfunction cnn() {\n  var model = tf.sequential();\n  model.add(tf.layers.conv2d({\n    inputShape: [28, 28, 1],\n    kernelSize: 5,\n    filters: 8,\n    strides: 1,\n    activation: 'relu',\n    kernelInitializer: 'varianceScaling'\n  }));\n  model.add(tf.layers.maxPooling2d({\n    poolSize: [2, 2],\n    strides: [2, 2]\n  }));\n  model.add(tf.layers.conv2d({\n    kernelSize: 5,\n    filters: 16,\n    strides: 1,\n    activation: 'relu',\n    kernelInitializer: 'varianceScaling'\n  }));\n  model.add(tf.layers.maxPooling2d({\n    poolSize: [2, 2],\n    strides: [2, 2]\n  }));\n  model.add(tf.layers.flatten());\n  model.add(tf.layers.dense({\n    units: 10,\n    kernelInitializer: 'varianceScaling',\n    activation: 'softmax'\n  }));\n  return model;\n}\n\nfunction train() {\n  return _train.apply(this, arguments);\n}\n\nfunction _train() {\n  _train = _asyncToGenerator(\n  /*#__PURE__*/\n  regeneratorRuntime.mark(function _callee3() {\n    var BATCH_SIZE, TRAIN_BATCHES, TEST_BATCH_SIZE, TEST_ITERATION_FREQUENCY, model, LEARNING_RATE, optimizer, data, load, _load, mnist, _mnist, i, batch, testBatch, validationData, history, _i, loss, accuracy, myDate;\n\n    return regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            _mnist = function _ref4() {\n              _mnist = _asyncToGenerator(\n              /*#__PURE__*/\n              regeneratorRuntime.mark(function _callee2() {\n                return regeneratorRuntime.wrap(function _callee2$(_context2) {\n                  while (1) {\n                    switch (_context2.prev = _context2.next) {\n                      case 0:\n                        _context2.next = 2;\n                        return load();\n\n                      case 2:\n                        console.log(\"Data loaded!\");\n\n                      case 3:\n                      case \"end\":\n                        return _context2.stop();\n                    }\n                  }\n                }, _callee2);\n              }));\n              return _mnist.apply(this, arguments);\n            };\n\n            mnist = function _ref3() {\n              return _mnist.apply(this, arguments);\n            };\n\n            _load = function _ref2() {\n              _load = _asyncToGenerator(\n              /*#__PURE__*/\n              regeneratorRuntime.mark(function _callee() {\n                return regeneratorRuntime.wrap(function _callee$(_context) {\n                  while (1) {\n                    switch (_context.prev = _context.next) {\n                      case 0:\n                        data = new MnistData();\n                        _context.next = 3;\n                        return data.load();\n\n                      case 3:\n                      case \"end\":\n                        return _context.stop();\n                    }\n                  }\n                }, _callee);\n              }));\n              return _load.apply(this, arguments);\n            };\n\n            load = function _ref() {\n              return _load.apply(this, arguments);\n            };\n\n            BATCH_SIZE = 16;\n            TRAIN_BATCHES = 10;\n            TEST_BATCH_SIZE = 100;\n            TEST_ITERATION_FREQUENCY = 5;\n            model = cnn();\n            LEARNING_RATE = 0.15;\n            optimizer = tf.train.sgd(LEARNING_RATE);\n            model.compile({\n              optimizer: optimizer,\n              loss: 'categoricalCrossentropy',\n              metrics: ['accuracy']\n            });\n            _context3.next = 14;\n            return mnist();\n\n          case 14:\n            i = 0;\n\n          case 15:\n            if (!(i < TRAIN_BATCHES)) {\n              _context3.next = 33;\n              break;\n            }\n\n            console.log(\"hihi2\");\n            batch = data.nextTrainBatch(BATCH_SIZE);\n            testBatch = void 0;\n            validationData = void 0; // Every few batches test the accuracy of the mode.\n\n            if (i % TEST_ITERATION_FREQUENCY === 0 && i > 0) {\n              testBatch = data.nextTestBatch(TEST_BATCH_SIZE);\n              validationData = [testBatch.xs.reshape([TEST_BATCH_SIZE, 28, 28, 1]), testBatch.labels];\n            } // The entire dataset doesn't fit into memory so we call fit repeatedly\n            // with batches.\n\n\n            _context3.next = 23;\n            return model.fit(batch.xs.reshape([BATCH_SIZE, 28, 28, 1]), batch.labels, {\n              batchSize: BATCH_SIZE,\n              validationData: validationData,\n              epochs: 1\n            });\n\n          case 23:\n            history = _context3.sent;\n\n            for (_i = 0; _i < history.history.loss.length; _i++) {\n              loss = history.history.loss[_i];\n              accuracy = history.history.acc[_i];\n              console.log(loss);\n              console.log(accuracy);\n            }\n\n            batch.xs.dispose();\n            batch.labels.dispose();\n\n            if (testBatch != null) {\n              testBatch.xs.dispose();\n              testBatch.labels.dispose();\n            }\n\n            _context3.next = 30;\n            return tf.nextFrame();\n\n          case 30:\n            i++;\n            _context3.next = 15;\n            break;\n\n          case 33:\n            myDate = new Date(); //await model.save('downloads://minist-'+myDate.toLocaleString().toString());\n            //await model.save('indexeddb://my-model-1');\n\n          case 34:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n  return _train.apply(this, arguments);\n}\n\nexport { train };",{"version":3,"sources":["C:\\Users\\1\\Desktop\\tensorflowjs\\face-api-demo-vue-master\\src\\utils\\cnntfjs.js"],"names":["tf","require","MnistData","cnn","model","sequential","add","layers","conv2d","inputShape","kernelSize","filters","strides","activation","kernelInitializer","maxPooling2d","poolSize","flatten","dense","units","train","load","mnist","console","log","data","BATCH_SIZE","TRAIN_BATCHES","TEST_BATCH_SIZE","TEST_ITERATION_FREQUENCY","LEARNING_RATE","optimizer","sgd","compile","loss","metrics","i","batch","nextTrainBatch","testBatch","validationData","nextTestBatch","xs","reshape","labels","fit","batchSize","epochs","history","length","accuracy","acc","dispose","nextFrame","myDate","Date"],"mappings":";;;AAAA,IAAMA,EAAE,GAAGC,OAAO,CAAC,kBAAD,CAAlB;;AACA,OAAOC,SAAP;;AAEA,SAASC,GAAT,GAAe;AACX,MAAMC,KAAK,GAAGJ,EAAE,CAACK,UAAH,EAAd;AACAD,EAAAA,KAAK,CAACE,GAAN,CAAUN,EAAE,CAACO,MAAH,CAAUC,MAAV,CAAiB;AACzBC,IAAAA,UAAU,EAAE,CAAC,EAAD,EAAK,EAAL,EAAS,CAAT,CADa;AAEzBC,IAAAA,UAAU,EAAE,CAFa;AAGzBC,IAAAA,OAAO,EAAE,CAHgB;AAIzBC,IAAAA,OAAO,EAAE,CAJgB;AAKzBC,IAAAA,UAAU,EAAE,MALa;AAMzBC,IAAAA,iBAAiB,EAAE;AANM,GAAjB,CAAV;AAQAV,EAAAA,KAAK,CAACE,GAAN,CAAUN,EAAE,CAACO,MAAH,CAAUQ,YAAV,CAAuB;AAACC,IAAAA,QAAQ,EAAE,CAAC,CAAD,EAAI,CAAJ,CAAX;AAAmBJ,IAAAA,OAAO,EAAE,CAAC,CAAD,EAAI,CAAJ;AAA5B,GAAvB,CAAV;AACAR,EAAAA,KAAK,CAACE,GAAN,CAAUN,EAAE,CAACO,MAAH,CAAUC,MAAV,CAAiB;AACzBE,IAAAA,UAAU,EAAE,CADa;AAEzBC,IAAAA,OAAO,EAAE,EAFgB;AAGzBC,IAAAA,OAAO,EAAE,CAHgB;AAIzBC,IAAAA,UAAU,EAAE,MAJa;AAKzBC,IAAAA,iBAAiB,EAAE;AALM,GAAjB,CAAV;AAOAV,EAAAA,KAAK,CAACE,GAAN,CAAUN,EAAE,CAACO,MAAH,CAAUQ,YAAV,CAAuB;AAACC,IAAAA,QAAQ,EAAE,CAAC,CAAD,EAAI,CAAJ,CAAX;AAAmBJ,IAAAA,OAAO,EAAE,CAAC,CAAD,EAAI,CAAJ;AAA5B,GAAvB,CAAV;AACAR,EAAAA,KAAK,CAACE,GAAN,CAAUN,EAAE,CAACO,MAAH,CAAUU,OAAV,EAAV;AACAb,EAAAA,KAAK,CAACE,GAAN,CAAUN,EAAE,CAACO,MAAH,CAAUW,KAAV,CACR;AAACC,IAAAA,KAAK,EAAE,EAAR;AAAYL,IAAAA,iBAAiB,EAAE,iBAA/B;AAAkDD,IAAAA,UAAU,EAAE;AAA9D,GADQ,CAAV;AAEA,SAAOT,KAAP;AACD;;SAGYgB,K;;;;;;;0BAAf;AAAA,qHAiBiBC,IAjBjB,SAsBiBC,KAtBjB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sCAsBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+BACQD,IAAI,EADZ;;AAAA;AAEEE,wBAAAA,OAAO,CAACC,GAAR,CAAY,cAAZ;;AAFF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,eAtBF;AAAA;AAAA;;AAsBiBF,YAAAA,KAtBjB;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA,sCAiBE;AAAA;AAAA;AAAA;AAAA;AACEG,wBAAAA,IAAI,GAAG,IAAIvB,SAAJ,EAAP;AADF;AAAA,+BAEQuB,IAAI,CAACJ,IAAL,EAFR;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,eAjBF;AAAA;AAAA;;AAiBiBA,YAAAA,IAjBjB;AAAA;AAAA;;AACQK,YAAAA,UADR,GACqB,EADrB;AAEQC,YAAAA,aAFR,GAEwB,EAFxB;AAGQC,YAAAA,eAHR,GAG0B,GAH1B;AAIQC,YAAAA,wBAJR,GAImC,CAJnC;AAMQzB,YAAAA,KANR,GAMgBD,GAAG,EANnB;AAOQ2B,YAAAA,aAPR,GAOwB,IAPxB;AAQQC,YAAAA,SARR,GAQoB/B,EAAE,CAACoB,KAAH,CAASY,GAAT,CAAaF,aAAb,CARpB;AASE1B,YAAAA,KAAK,CAAC6B,OAAN,CAAc;AACZF,cAAAA,SAAS,EAAEA,SADC;AAEZG,cAAAA,IAAI,EAAE,yBAFM;AAGZC,cAAAA,OAAO,EAAE,CAAC,UAAD;AAHG,aAAd;AATF;AAAA,mBA2BQb,KAAK,EA3Bb;;AAAA;AA6BWc,YAAAA,CA7BX,GA6Be,CA7Bf;;AAAA;AAAA,kBA6BkBA,CAAC,GAAGT,aA7BtB;AAAA;AAAA;AAAA;;AA8BIJ,YAAAA,OAAO,CAACC,GAAR,CAAY,OAAZ;AACMa,YAAAA,KA/BV,GA+BkBZ,IAAI,CAACa,cAAL,CAAoBZ,UAApB,CA/BlB;AAiCQa,YAAAA,SAjCR;AAkCQC,YAAAA,cAlCR,WAmCI;;AACA,gBAAIJ,CAAC,GAAGP,wBAAJ,KAAiC,CAAjC,IAAsCO,CAAC,GAAG,CAA9C,EAAkD;AAChDG,cAAAA,SAAS,GAAGd,IAAI,CAACgB,aAAL,CAAmBb,eAAnB,CAAZ;AACAY,cAAAA,cAAc,GAAG,CACfD,SAAS,CAACG,EAAV,CAAaC,OAAb,CAAqB,CAACf,eAAD,EAAkB,EAAlB,EAAsB,EAAtB,EAA0B,CAA1B,CAArB,CADe,EACqCW,SAAS,CAACK,MAD/C,CAAjB;AAGD,aAzCL,CA2CI;AACA;;;AA5CJ;AAAA,mBA6C0BxC,KAAK,CAACyC,GAAN,CAClBR,KAAK,CAACK,EAAN,CAASC,OAAT,CAAiB,CAACjB,UAAD,EAAa,EAAb,EAAiB,EAAjB,EAAqB,CAArB,CAAjB,CADkB,EACyBW,KAAK,CAACO,MAD/B,EAElB;AAACE,cAAAA,SAAS,EAAEpB,UAAZ;AAAwBc,cAAAA,cAAc,EAAdA,cAAxB;AAAwCO,cAAAA,MAAM,EAAE;AAAhD,aAFkB,CA7C1B;;AAAA;AA6CUC,YAAAA,OA7CV;;AAgDI,iBAAQZ,EAAR,GAAU,CAAV,EAAaA,EAAC,GAACY,OAAO,CAACA,OAAR,CAAgBd,IAAhB,CAAqBe,MAApC,EAA4Cb,EAAC,EAA7C,EAAgD;AAC1CF,cAAAA,IAD0C,GACnCc,OAAO,CAACA,OAAR,CAAgBd,IAAhB,CAAqBE,EAArB,CADmC;AAE1Cc,cAAAA,QAF0C,GAE/BF,OAAO,CAACA,OAAR,CAAgBG,GAAhB,CAAoBf,EAApB,CAF+B;AAG9Cb,cAAAA,OAAO,CAACC,GAAR,CAAYU,IAAZ;AACAX,cAAAA,OAAO,CAACC,GAAR,CAAY0B,QAAZ;AACD;;AACDb,YAAAA,KAAK,CAACK,EAAN,CAASU,OAAT;AACAf,YAAAA,KAAK,CAACO,MAAN,CAAaQ,OAAb;;AACA,gBAAIb,SAAS,IAAI,IAAjB,EAAuB;AACrBA,cAAAA,SAAS,CAACG,EAAV,CAAaU,OAAb;AACAb,cAAAA,SAAS,CAACK,MAAV,CAAiBQ,OAAjB;AACD;;AA3DL;AAAA,mBA6DUpD,EAAE,CAACqD,SAAH,EA7DV;;AAAA;AA6BqCjB,YAAAA,CAAC,EA7BtC;AAAA;AAAA;;AAAA;AA+DMkB,YAAAA,MA/DN,GA+De,IAAIC,IAAJ,EA/Df,EAgEE;AACA;;AAjEF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,G;;;;AAoEA,SACEnC,KADF","sourcesContent":["const tf = require(\"@tensorflow/tfjs\");\r\nimport MnistData from \"./data\"\r\n\r\nfunction cnn() {\r\n    const model = tf.sequential();\r\n    model.add(tf.layers.conv2d({\r\n      inputShape: [28, 28, 1],\r\n      kernelSize: 5,\r\n      filters: 8,\r\n      strides: 1,\r\n      activation: 'relu',\r\n      kernelInitializer: 'varianceScaling'\r\n    }));\r\n    model.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\r\n    model.add(tf.layers.conv2d({\r\n      kernelSize: 5,\r\n      filters: 16,\r\n      strides: 1,\r\n      activation: 'relu',\r\n      kernelInitializer: 'varianceScaling'\r\n    }));\r\n    model.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\r\n    model.add(tf.layers.flatten());\r\n    model.add(tf.layers.dense(\r\n      {units: 10, kernelInitializer: 'varianceScaling', activation: 'softmax'}));\r\n    return model;\r\n  }\r\n\r\n\r\nasync function train() {\r\n  const BATCH_SIZE = 16;\r\n  const TRAIN_BATCHES = 10;\r\n  const TEST_BATCH_SIZE = 100;\r\n  const TEST_ITERATION_FREQUENCY = 5;\r\n\r\n  const model = cnn();\r\n  const LEARNING_RATE = 0.15;\r\n  const optimizer = tf.train.sgd(LEARNING_RATE);\r\n  model.compile({\r\n    optimizer: optimizer,\r\n    loss: 'categoricalCrossentropy',\r\n    metrics: ['accuracy'],\r\n  });\r\n\r\n  let data;\r\n\r\n  async function load() {\r\n    data = new MnistData();\r\n    await data.load();\r\n  }\r\n\r\n  async function mnist() {\r\n    await load();\r\n    console.log(\"Data loaded!\");\r\n  }\r\n\r\n  await mnist()\r\n\r\n  for (let i = 0; i < TRAIN_BATCHES; i++) {\r\n    console.log(\"hihi2\")\r\n    const batch = data.nextTrainBatch(BATCH_SIZE);\r\n\r\n    let testBatch;\r\n    let validationData;\r\n    // Every few batches test the accuracy of the mode.\r\n    if (i % TEST_ITERATION_FREQUENCY === 0 && i > 0 ) {\r\n      testBatch = data.nextTestBatch(TEST_BATCH_SIZE);\r\n      validationData = [\r\n        testBatch.xs.reshape([TEST_BATCH_SIZE, 28, 28, 1]), testBatch.labels\r\n      ];\r\n    }\r\n\r\n    // The entire dataset doesn't fit into memory so we call fit repeatedly\r\n    // with batches.\r\n    const history = await model.fit(\r\n        batch.xs.reshape([BATCH_SIZE, 28, 28, 1]), batch.labels,\r\n        {batchSize: BATCH_SIZE, validationData, epochs: 1});\r\n    for(let i=0; i<history.history.loss.length; i++){\r\n      let loss = history.history.loss[i];\r\n      let accuracy = history.history.acc[i];\r\n      console.log(loss)\r\n      console.log(accuracy)\r\n    }\r\n    batch.xs.dispose();\r\n    batch.labels.dispose();\r\n    if (testBatch != null) {\r\n      testBatch.xs.dispose();\r\n      testBatch.labels.dispose();\r\n    }\r\n\r\n    await tf.nextFrame();\r\n  }\r\n  let myDate = new Date()\r\n  //await model.save('downloads://minist-'+myDate.toLocaleString().toString());\r\n  //await model.save('indexeddb://my-model-1');\r\n}\r\n\r\nexport{\r\n  train as train\r\n}"]}]}